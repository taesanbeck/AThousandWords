{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://huggingface.co/deepset/bert-large-uncased-whole-word-masking-squad2\n",
    "from transformers import AutoModelForQuestionAnswering, AutoTokenizer, pipeline\n",
    "\n",
    "def get_answer(question, context):\n",
    "    model_name = \"deepset/bert-large-uncased-whole-word-masking-squad2\"\n",
    "    nlp = pipeline('question-answering', model=model_name, tokenizer=model_name)\n",
    "    QA_input = {\n",
    "        'question': question,\n",
    "        'context': context\n",
    "    }\n",
    "    res = nlp(QA_input)\n",
    "    return res['answer']\n",
    "\n",
    "# This is were the CV Model would output\n",
    "objects = ['Dog', 'Ball', 'Person', 'House', 'Tree']\n",
    "objects_str = ', '.join(objects)\n",
    "\n",
    "# Test the function\n",
    "question = 'What is in the photograph?'\n",
    "context = f'The photograph contains {objects_str}'\n",
    "answer = get_answer(question, context)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There is a Dog, a Ball, a Person, a House, and a Tree in the photo.\n"
     ]
    }
   ],
   "source": [
    "# Make the answer more human-like\n",
    "# This line of code formats the model's answer in a more \"human-like\" sentence structure. \n",
    "# It adds an \"a\" before each object and an \"and\" before the last object.\n",
    "# 1. It begins the sentence with \"There is \".\n",
    "# 2. The '.join([f'a {obj}' for obj in answer.split(', ')[:-1]])' part takes all objects from the answer except the last one,\n",
    "#    prepends each with \"a \", and joins them together with commas.\n",
    "# 3. The '+ ', and a ' + answer.split(', ')[-1]' part takes the last object, prepends it with \"and a \", and adds it to the sentence.\n",
    "# 4. Finally, it appends \" in the photo.\" to the end of the sentence.\n",
    "human_like_answer = 'There is ' + ', '.join([f'a {obj}' for obj in answer.split(', ')[:-1]]) + ', and a ' + answer.split(', ')[-1] + ' in the photo.'\n",
    "print(human_like_answer)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "GPT2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using pad_token, but it is not set yet.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Describe a scene where a dog, cat, person, girl, boy, house, tree are interacting with each other.\n",
      "\n",
      "The scene is a scene where a dog, cat, person, girl, boy, house, tree are interacting with each other.\n",
      "\n",
      "The scene is a scene where a dog, cat, person, girl, boy, house, tree are interacting with each other\n"
     ]
    }
   ],
   "source": [
    "from transformers import GPT2Tokenizer, GPT2LMHeadModel\n",
    "\n",
    "def describe_image(labels):\n",
    "    # Initialize GPT2 model and tokenizer\n",
    "    tokenizer = GPT2Tokenizer.from_pretrained('gpt2', padding_side='left')\n",
    "    model = GPT2LMHeadModel.from_pretrained('gpt2')\n",
    "\n",
    "    # If 'pad_token' is not defined, set it as 'eos_token'\n",
    "    if tokenizer.pad_token is None:\n",
    "        tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "    # Join labels with commas and turn into a string\n",
    "    label_string = \", \".join(labels)\n",
    "    # Add a prompt for the GPT2 model to generate a description from\n",
    "    prompt = f\"Describe a scene where a {label_string} are interacting with each other.\"\n",
    "    # Encode the prompt and create attention_mask\n",
    "    inputs = tokenizer.encode(prompt, return_tensors='pt', add_special_tokens=True)\n",
    "    attention_mask = inputs.ne(tokenizer.pad_token_id).float()\n",
    "    # Generate a text from the prompt. Adjust temperature up for more randomness, and max_length for longer outputs. top_p is top probability threshhold for words. ex: 0.5 would only consider the smallest possible set of words whos cumulative probability exceeds 0.5\n",
    "    outputs = model.generate(inputs, max_length=80, temperature=0.4, do_sample=True, top_p=0.4, attention_mask=attention_mask, pad_token_id=tokenizer.eos_token_id)\n",
    "    # Decode the output\n",
    "    output_text = tokenizer.decode(outputs[0])\n",
    "    return output_text\n",
    "\n",
    "# Example usage:\n",
    "labels = [\"dog\", \"cat\", \"person\", \"girl\", \"boy\", \"house\", \"tree\"]\n",
    "print(describe_image(labels))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
